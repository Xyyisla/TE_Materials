{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 特征工程：方差阈值VT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 从sklearn.feature_selection中导入VarianceThreshold:\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(threshold=0.16)\n",
    "\n",
    "df = pd.read_csv('last_data.csv')\n",
    "\n",
    "Before_Features = df.drop(['formula','composition','ZT','Unnamed: 0'], axis =1)\n",
    "'''\n",
    "我们像任何其他Scikit-learn估计器一样初始化它。阈值的默认值总是0。\n",
    "而且，估计器显然只对数字数据有效，如果数据中存在分类特征，估计器就会抛出错误。\n",
    "所以我们需要把数字特性子集放到另一个dataframe中\n",
    "'''\n",
    "The_mat_num = Before_Features.select_dtypes(include='number')\n",
    "print(The_mat_num.shape)\n",
    "\n",
    "# 将估计量与数据进行拟合\n",
    "transformed = vt.fit_transform(The_mat_num)\n",
    "# print(transformed)\n",
    "'''\n",
    "直接调用fit_transform将以numpy数组的形式返回dataframe，并删除特性。\n",
    "但有时，我们不希望得到那种格式的结果，因为列名将被删除。考虑选择:\n",
    "'''\n",
    "_ = vt.fit(The_mat_num)\n",
    "mask = vt.get_support()\n",
    "'''\n",
    "首先，我们将估计器与数据相匹配，并调用它的get_support()方法。\n",
    "对于未删除的列，它返回一个为真值的布尔类型的掩码。然后我们可以使用这个掩码来像这样划分数据:\n",
    "'''\n",
    "The_mat_reduced = The_mat_num.loc[:, mask]\n",
    "print(The_mat_num.shape)\n",
    "'''\n",
    "以上特征均具有不同的中位数，四分位数和范围完全不同的分布。 我们无法将这些功能相互比较。\n",
    "我们可以使用的一种方法是通过将所有特征除以均值来对其进行归一化\n",
    "'''\n",
    "normalized_df = The_mat_num / The_mat_num.mean()\n",
    "# print(normalized_df.head())\n",
    "# print(normalized_df.var())\n",
    "\n",
    "vt = VarianceThreshold(threshold=.16)\n",
    "\n",
    "# Fit\n",
    "_ = vt.fit(normalized_df)\n",
    "\n",
    "# Get the mask\n",
    "mask = vt.get_support()\n",
    "\n",
    "# Subset the DataFrame\n",
    "te_final = The_mat_num.loc[:, mask]\n",
    "te_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 特征工程：皮尔逊相关系性\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # 相关列的所有名称的集合\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # 我们感兴趣的是绝对值\n",
    "                colname = corr_matrix.columns[i]  # 获取列的名称获取列的名称\n",
    "                col_corr.add(colname)\n",
    "    af_corr = dataset.drop(col_corr,axis=1)\n",
    "    return af_corr\n",
    "\n",
    "\n",
    "af_both = correlation(te_final, 0.64)\n",
    "print('Shape sould be:', af_both.shape)\n",
    "\n",
    "A_cor = af_both.corr()\n",
    "plt.figure(figsize=(25,20))\n",
    "sns.heatmap(A_cor,cmap=plt.cm.CMRmap_r,annot=False)\n",
    "plt.title(\"Correlation\",size = 20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,KFold\n",
    "KF = KFold(n_splits = 10)\n",
    "Y = df['ZT']\n",
    "X_train, X_test, y_train, y_test = KF.train_test_split(af_both, Y, test_size=0.20, random_state=415)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "N_epoch = 1000\n",
    "best_loss = 9999\n",
    "par = tqdm(range(N_epoch))\n",
    "for i in par:\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_v = 0\n",
    "\n",
    "    for data, label in train_loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pre = model(data)\n",
    "        loss = loss_f(pre, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss = epoch_loss + loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in valid_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            pre = model(data)\n",
    "            loss = loss_f(pre, data)\n",
    "            epoch_loss_v = epoch_loss_v + loss.item()\n",
    "\n",
    "    if epoch_loss_v / len(valid_loader) < best_loss:\n",
    "        # print('Save Model..')\n",
    "        best_loss = epoch_loss_v / len(valid_loader)\n",
    "        torch.save(model.encoder.state_dict(), './result/Encoder-72.pth')\n",
    "        torch.save(model.state_dict(), './result/Auto-Encoder-72.pth')\n",
    "\n",
    "    train_loss_list.append(epoch_loss)\n",
    "    valid_loss_list.append(epoch_loss_v)\n",
    "    np.savetxt('./result/train_loss.txt', train_loss_list, fmt='%.5f')\n",
    "    np.savetxt('./result/valid_loss.txt', valid_loss_list, fmt='%.5f')\n",
    "\n",
    "    par.set_description_str(f'Epoch: {i + 1:02}')\n",
    "    par.set_postfix_str(f'Train: {epoch_loss / len(train_loader):.5f} | Valid: {epoch_loss_v / len(valid_loader):.5f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device='cpu'\n",
    "output_dim=72\n",
    "model.load_state_dict(torch.load('./result/Auto-Encoder-72.pth'))\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "\n",
    "    count=0\n",
    "    for data,label in tqdm(train_loader):\n",
    "        data=data.to(device)\n",
    "        label=label.to(device).detach().numpy()\n",
    "        pred=model.encode(data)\n",
    "        if count==0:\n",
    "            train_data=pred\n",
    "            train_label=label\n",
    "        else:\n",
    "            train_data=np.concatenate([train_data,pred],axis=0)\n",
    "            train_label=np.concatenate([train_label,label],axis=0)\n",
    "        count+=1\n",
    "\n",
    "    count=0\n",
    "    for data,label in tqdm(valid_loader):\n",
    "        data=data.to(device)\n",
    "        label=label.to(device).detach().numpy()\n",
    "        pred=model.encode(data).detach().numpy()\n",
    "\n",
    "        if count==0:\n",
    "            valid_data=pred\n",
    "            valid_label=label\n",
    "        else:\n",
    "            valid_data=np.concatenate([valid_data,pred],\n",
    "                                      axis=0)\n",
    "            valid_label=np.concatenate([valid_label,label],\n",
    "                                       axis=0)\n",
    "        count+=1\n",
    "df_train=pd.DataFrame(train_data,\n",
    "                      columns=[f'feature {i}' for i in range(1,output_dim+1)])\n",
    "df_train['ZT']=train_label\n",
    "\n",
    "df_valid=pd.DataFrame(valid_data,\n",
    "                      columns=[f'feature {i}' for i in range(1,output_dim+1)])\n",
    "df_valid['ZT']=valid_label\n",
    "\n",
    "print(df_train.shape,df_valid.shape)\n",
    "df_train.to_csv(f'./result/Train-Reduced-Feature-{output_dim}.csv')\n",
    "df_valid.to_csv(f'./result/Test-Reduced-Feature-{output_dim}.csv')\n",
    "print('Save data..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "model_lgb = LGBMRegressor(boosting_type = \"gbdt\",\n",
    "                      num_leaves = '128',\n",
    "                      objective = 'regression',\n",
    "                      max_depth = 8,\n",
    "                      learning_rate = 0.03,\n",
    "                      subsample_freq = 1,\n",
    "                      subsample = 0.9,\n",
    "                      bagging_seed = 11,\n",
    "                      metric = 'mae',\n",
    "                      verbosity = -1,\n",
    "                      reg_alpha = 0.1,\n",
    "                      colsample_bytree = 1.0,\n",
    "                      n_estimators = 1000,\n",
    "                      verbose = 2000)\n",
    "model_lgb.fit(x_train,y_train)\n",
    "pred=model_lgb.predict(x_test)\n",
    "r2_score(y_test,pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 准备\n",
    "df1_0 = pd.read_csv('final_data.csv')\n",
    "#, 'Unnamed: 0'  #last_data:7130    ,\n",
    "df2_0 = df1_0.drop(['formula', 'composition'], axis=1)\n",
    "x_col = df2_0.columns.drop(['ZT'])\n",
    "X = df2_0[x_col]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 在此训练回归模型\n",
    "df1=pd.read_csv('./result/Train-Reduced-Feature-72.csv')\n",
    "print(df1.shape,df1.columns)\n",
    "train_data=df1.drop(['ZT','Unnamed: 0'],axis=1)\n",
    "df2 = pd.read_csv('./result/Test-Reduced-Feature-72.csv')\n",
    "test_data = df2.drop(['ZT','Unnamed: 0'], axis=1)\n",
    "\n",
    "\n",
    "model_lgb = LGBMRegressor(boosting_type=\"gbdt\",\n",
    "                              num_leaves='128',\n",
    "                              objective='regression',\n",
    "                              max_depth=8,\n",
    "                              learning_rate=0.03,\n",
    "                              subsample_freq=1,\n",
    "                              subsample=0.9,\n",
    "                              bagging_seed=11,\n",
    "                              metric='mae',\n",
    "                              verbosity=-1,\n",
    "                              reg_alpha=0.1,\n",
    "                              colsample_bytree=1.0,\n",
    "                              n_estimators=1000,\n",
    "                              verbose=2000)\n",
    "\n",
    "model_lgb.fit(train_data, df1['ZT'])\n",
    "\n",
    "pred = model_lgb.predict(test_data)\n",
    "\n",
    "# model_rf = RandomForestRegressor(n_estimators=300)\n",
    "# model_rf.fit(train_data, df1['ZT'])\n",
    "# pred = model_rf.predict(test_data)\n",
    "\n",
    "print(f'D{72} R2 Score:{r2_score(df2.ZT, pred ):.4f}')\n",
    "\n",
    "\n",
    "model = AutoEncoder(input_dim=146,\n",
    "                    output_dim=72)\n",
    "model.load_state_dict(torch.load('./result/Auto-Encoder-72.pth'))\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "loss_f = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(feature_scaled))):\n",
    "        input_feature = feature_scaled[i].reshape(-1, 146)\n",
    "        input_feature = torch.Tensor(input_feature)\n",
    "\n",
    "        pred = model(input_feature)\n",
    "        loss = loss_f(pred, input_feature).item()\n",
    "        loss_list.append(loss)\n",
    "        if loss > 0.0005:\n",
    "            label_list.append(0)\n",
    "        else:\n",
    "            label_list.append(1)\n",
    "\n",
    "        # encode 146->72 feature\n",
    "        encode_feature = model.encode(input_feature).cpu().detach().numpy()\n",
    "\n",
    "        zt_pred=model_lgb.predict(encode_feature)[0]\n",
    "        zt_pred_list.append(zt_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}